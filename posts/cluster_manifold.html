<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2023-03-16 Thu 15:57 -->
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Clustering of a local manifold</title>
<meta name="author" content="Luca Zaninotto" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../style.css"/>
</head>
<body>
<div id="preamble" class="status">
<nav><a id="navbar-home" href="../index.html">üè°  Home</a></nav>
</div>
<div id="content" class="content">
<h1 class="title">Clustering of a local manifold</h1>
<div id="outline-container-org43bf274" class="outline-2">
<h2 id="org43bf274">Intro</h2>
<div class="outline-text-2" id="text-org43bf274">
<p>
This is the product of a machine learning course at my
university. The task was to explore the Machine Learning research
space and find something interesting to analyze. I found <a href="https://arxiv.org/abs/1908.05968">N2D: (Not
Too) Deep Clustering via Clustering the Local Manifold of an
Autoencoded Embedding</a>, a new approach in clustering: essentially
preprocess the data trough an autoencoder to cluster inputs more
easily. I made an original implementation with <a href="https://pytorch.org/">pytorch</a> and I ran it
against the <a href="https://it.wikipedia.org/wiki/MNIST_database">MNIST dataset</a>, obtaining comparable results to the ones
obtained by the original authors. Here's the static version of the
notebook you can find <a href="https://colab.research.google.com/drive/11j16mhZbaPT_2RA7rb6F1c5JRcn2RMYe?usp=share_link">here</a>.
</p>
</div>
</div>

<div id="outline-container-org67ad758" class="outline-2">
<h2 id="org67ad758">Imports</h2>
<div class="outline-text-2" id="text-org67ad758">
<p>
In this section we'll import all the necessary libraries, such as
</p>
<ul class="org-ul">
<li>numpy for matrix manipulation and arithmetic</li>
<li>pytorch, the main library for machine learning we'll use</li>
<li>matplotlib to plot our results</li>
</ul>
</div>

<div id="outline-container-org02669bf" class="outline-3">
<h3 id="org02669bf">The libraries</h3>
<div class="outline-text-3" id="text-org02669bf">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #505050;"># </span><span style="color: #505050;">Matplotlib and numpy</span>
<span style="color: #5317ac;">import</span> matplotlib.pyplot <span style="color: #5317ac;">as</span> plt
<span style="color: #5317ac;">import</span> numpy <span style="color: #5317ac;">as</span> np

<span style="color: #505050;"># </span><span style="color: #505050;">torch, datasets and its utilities</span>
<span style="color: #5317ac;">import</span> torch
<span style="color: #5317ac;">import</span> torch.nn <span style="color: #5317ac;">as</span> nn
<span style="color: #5317ac;">import</span> torch.optim <span style="color: #5317ac;">as</span> optim
<span style="color: #5317ac;">import</span> torchvision
<span style="color: #5317ac;">import</span> random

<span style="color: #505050;"># </span><span style="color: #505050;">switch to gpu if available</span>
<span style="color: #00538b;">device</span> = torch.device(<span style="color: #2544bb;">"cuda"</span> <span style="color: #5317ac;">if</span> torch.cuda.is_available() <span style="color: #5317ac;">else</span> <span style="color: #2544bb;">"cpu"</span>)
<span style="color: #8f0075;">print</span>(<span style="color: #2544bb;">'Using device: '</span>, device)

<span style="color: #505050;"># </span><span style="color: #505050;">setting random seeds for reproducibility</span>
torch.manual_seed(0)
np.random.seed(0)
random.seed(0)

</pre>
</div>
</div>
</div>
<div id="outline-container-orgf85980f" class="outline-3">
<h3 id="orgf85980f">The dataset</h3>
<div class="outline-text-3" id="text-orgf85980f">
<p>
In the following section we'll import our train and
test datasets, in order to do so we need the loaders for both the test
and train data
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #5317ac;">from</span> torchvision <span style="color: #5317ac;">import</span> datasets <span style="color: #5317ac;">as</span> dts
<span style="color: #5317ac;">from</span> torchvision.transforms <span style="color: #5317ac;">import</span> ToTensor 

<span style="color: #00538b;">train_data</span> = dts.MNIST(
    root = <span style="color: #2544bb;">'data'</span>,
    train = <span style="color: #0000c0;">True</span>,                         
    transform = ToTensor(), 
    download = <span style="color: #0000c0;">True</span>,            
)

test_data = dts.MNIST(
    root = <span style="color: #2544bb;">'data'</span>, 
    train = <span style="color: #0000c0;">False</span>, 
    transform = ToTensor()
)

train_loader = torch.utils.data.DataLoader(
    train_data, 
    batch_size=128, 
    shuffle=<span style="color: #0000c0;">True</span>,
)

test_loader = torch.utils.data.DataLoader(
    test_data, 
    batch_size=20, 
    shuffle=<span style="color: #0000c0;">True</span>
)

</pre>
</div>
</div>
</div>
</div>


<div id="outline-container-org3cac973" class="outline-2">
<h2 id="org3cac973">The autoencoder</h2>
<div class="outline-text-2" id="text-org3cac973">
<p>
As described in the paper, the first component of our model is the
autoencoder, takes as input the image as an array of dimensionality
(1, 784) and ideally should output the same image. The rationale
behind the structuring of this component in such a way (2 bottleneck
layers of 500 units, an intermediate hidden layer of 2000 units and
then the dual decoder) is described in the report of the project.
</p>
</div>

<div id="outline-container-orgb6969a6" class="outline-3">
<h3 id="orgb6969a6">The encoder</h3>
<div class="outline-text-3" id="text-orgb6969a6">
<p>
The encoder component of the autoencoder is responsible to encode
the input data into vectors of different dimensionality, the
bottleneck (2 units of lower dimensionality as the input) force the
encoder to actually select the features it considers most important
from the raw data, the output is 2000 unit.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #5317ac;">class</span> <span style="color: #005a5f;">Encoder</span>(nn.Module):
    <span style="color: #5317ac;">def</span> <span style="color: #721045;">__init__</span>(<span style="color: #5317ac;">self</span>, shape):
        <span style="color: #8f0075;">super</span>().__init__()
        <span style="color: #5317ac;">self</span>.<span style="color: #00538b;">encoder_hidden_layer_1</span> = nn.Linear(
            in_features=shape, out_features=500
        )
        <span style="color: #5317ac;">self</span>.encoder_hidden_layer_2 = nn.Linear(
            in_features=500, out_features=500
        )
        <span style="color: #5317ac;">self</span>.encoder_output_layer = nn.Linear(
            in_features=500, out_features=2000
        )

    <span style="color: #5317ac;">def</span> <span style="color: #721045;">forward</span>(<span style="color: #5317ac;">self</span>, features):
        activation = <span style="color: #5317ac;">self</span>.encoder_hidden_layer_1(features)
        activation = torch.relu(activation)
        activation = <span style="color: #5317ac;">self</span>.encoder_hidden_layer_2(activation)
        activation = torch.relu(activation)
        code = <span style="color: #5317ac;">self</span>.encoder_output_layer(activation)
        code = torch.sigmoid(code)
        <span style="color: #5317ac;">return</span> code
</pre>
</div>
</div>
</div>

<div id="outline-container-org3215ba7" class="outline-3">
<h3 id="org3215ba7">The decoder</h3>
<div class="outline-text-3" id="text-org3215ba7">
<p>
Can be considered as the other side of the collaborative network
encoder-decoder, and has the job of better turn the encoding given
by the encoder into the initial data. Is very important in order to
train the encoder to find the best possible features to rapresent
in order to get the best possible encoding.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #5317ac;">class</span> <span style="color: #005a5f;">Decoder</span>(nn.Module):
    <span style="color: #5317ac;">def</span> <span style="color: #721045;">__init__</span>(<span style="color: #5317ac;">self</span>, shape):
        <span style="color: #8f0075;">super</span>().__init__()
        <span style="color: #5317ac;">self</span>.<span style="color: #00538b;">decoder_hidden_layer_1</span> = nn.Linear(
            in_features=2000, out_features=500
        )
        <span style="color: #5317ac;">self</span>.decoder_hidden_layer_2 = nn.Linear(
            in_features=500, out_features=500
        )
        <span style="color: #5317ac;">self</span>.decoder_output_layer = nn.Linear(
            in_features=500, out_features=shape
        )

    <span style="color: #5317ac;">def</span> <span style="color: #721045;">forward</span>(<span style="color: #5317ac;">self</span>, code):
        activation = <span style="color: #5317ac;">self</span>.decoder_hidden_layer_1(code)
        activation = torch.relu(activation)
        activation = <span style="color: #5317ac;">self</span>.decoder_hidden_layer_2(activation)
        activation = torch.relu(activation)
        activation = <span style="color: #5317ac;">self</span>.decoder_output_layer(activation)
        reconstructed = torch.sigmoid(activation)
        <span style="color: #5317ac;">return</span> reconstructed
</pre>
</div>
</div>
</div>
<div id="outline-container-orgc8af94c" class="outline-3">
<h3 id="orgc8af94c">The autoencoder</h3>
<div class="outline-text-3" id="text-orgc8af94c">
<p>
The resulting network is the combination of the two
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #5317ac;">class</span> <span style="color: #005a5f;">AE</span>(nn.Module):
    <span style="color: #5317ac;">def</span> <span style="color: #721045;">__init__</span>(<span style="color: #5317ac;">self</span>, shape):
        <span style="color: #8f0075;">super</span>().__init__()
        <span style="color: #5317ac;">self</span>.<span style="color: #00538b;">encoder</span> = Encoder(shape)
        <span style="color: #5317ac;">self</span>.<span style="color: #00538b;">decoder</span> = Decoder(shape)

    <span style="color: #5317ac;">def</span> <span style="color: #721045;">forward</span>(<span style="color: #5317ac;">self</span>, features):
        <span style="color: #00538b;">code</span> = <span style="color: #5317ac;">self</span>.encoder.forward(features)
        <span style="color: #00538b;">reconstructed</span> = <span style="color: #5317ac;">self</span>.decoder.forward(code)
        <span style="color: #5317ac;">return</span> reconstructed
</pre>
</div>
<p>
We can now instanciate our model
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #505050;"># </span><span style="color: #505050;">create a model from ~AE~ autoencoder class</span>
<span style="color: #505050;"># </span><span style="color: #505050;">load it to the specified device, either gpu or cpu</span>
<span style="color: #00538b;">model</span> = AE(784).to(device)

<span style="color: #505050;"># </span><span style="color: #505050;">create an optimizer object</span>
<span style="color: #505050;"># </span><span style="color: #505050;">Adam optimizer with learning rate 1e-3</span>
<span style="color: #00538b;">optimizer</span> = optim.Adam(model.parameters(), lr=1e-3)

<span style="color: #505050;"># </span><span style="color: #505050;">mean-squared error loss</span>
criterion = nn.MSELoss()
</pre>
</div>
<p>
and train it
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #00538b;">epochs</span> = 20
<span style="color: #505050;"># </span><span style="color: #505050;">Set this to False to load the weights of the last successful run</span>
<span style="color: #00538b;">train</span> = <span style="color: #0000c0;">True</span>
<span style="color: #00538b;">path</span> = <span style="color: #2544bb;">"weights.pt"</span>

<span style="color: #5317ac;">if</span> <span style="color: #00538b;">train</span>:
  <span style="color: #5317ac;">for</span> epoch <span style="color: #5317ac;">in</span> <span style="color: #8f0075;">range</span>(epochs):
    loss = 0
      <span style="color: #5317ac;">for</span> x, _ <span style="color: #5317ac;">in</span> <span style="color: #00538b;">train_loader</span>:
        x = x.view(-1, 784).to(device)
        optimizer.zero_grad()
        <span style="color: #00538b;">outputs</span> = model(x)
        <span style="color: #00538b;">train_loss</span> = criterion(outputs, x)
        train_loss.backward()
        optimizer.step()
        <span style="color: #00538b;">loss</span> += train_loss.item()

      <span style="color: #00538b;">loss</span> = loss / <span style="color: #8f0075;">len</span>(train_loader)
      <span style="color: #8f0075;">print</span>(<span style="color: #2544bb;">"epoch : {}/{}, loss = {:.6f}"</span>.<span style="color: #8f0075;">format</span>(epoch + 1, epochs, loss))
      torch.save(model.state_dict(), path)
<span style="color: #5317ac;">else</span>:
  model.load_state_dict(torch.load(path))
</pre>
</div>
</div>
</div>

<div id="outline-container-orgfac998a" class="outline-3">
<h3 id="orgfac998a">Test the autoencoder</h3>
<div class="outline-text-3" id="text-orgfac998a">
<p>
The autoencoder can now be tested in order to get a rough
understanding of weather it is good enough on reconstructing images
or not.  On top there are the initial images, and below the
reconstructed ones. As we can see the accuracy became pretty high.
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #00538b;">test_examples</span> = <span style="color: #0000c0;">None</span>
model.<span style="color: #8f0075;">eval</span>()

<span style="color: #505050;"># </span><span style="color: #505050;">Test it over the first data of the test dataset</span>
<span style="color: #5317ac;">with</span> torch.no_grad():
    <span style="color: #5317ac;">for</span> batch_features <span style="color: #5317ac;">in</span> <span style="color: #00538b;">test_loader</span>:
        batch_features = batch_features[0].to(device)
        <span style="color: #00538b;">test_examples</span> = batch_features.view(-1, 784)
        <span style="color: #00538b;">reconstruction</span> = model(test_examples)
        <span style="color: #5317ac;">break</span>

<span style="color: #505050;"># </span><span style="color: #505050;">Plot results</span>
<span style="color: #5317ac;">with</span> torch.no_grad():
    <span style="color: #00538b;">number</span> = 10
    plt.figure(figsize=(20, 4))
    <span style="color: #5317ac;">for</span> index <span style="color: #5317ac;">in</span> <span style="color: #8f0075;">range</span>(number):
        <span style="color: #505050;"># </span><span style="color: #505050;">display original</span>
        ax = plt.subplot(2, number, index + 1)
        plt.imshow(test_examples[index].cpu().numpy().reshape(28, 28))
        plt.gray()
        ax.get_xaxis().set_visible(<span style="color: #0000c0;">False</span>)
        ax.get_yaxis().set_visible(<span style="color: #0000c0;">False</span>)

        <span style="color: #505050;"># </span><span style="color: #505050;">display reconstruction</span>
        ax = plt.subplot(2, number, index + 1 + number)
        plt.imshow(reconstruction[index].cpu().numpy().reshape(28, 28))
        plt.gray()
        ax.get_xaxis().set_visible(<span style="color: #0000c0;">False</span>)
        ax.get_yaxis().set_visible(<span style="color: #0000c0;">False</span>)
        plt.show()
</pre>
</div>
</div>
</div>

<div id="outline-container-org524ea18" class="outline-3">
<h3 id="org524ea18">Passing to the manifold learners</h3>
<div class="outline-text-3" id="text-org524ea18">
<p>
What we're really interested on is the output of the encoder
layer. So let's split the autoencoder component and provider its
output as input for the next component: the manifold learner.
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #00538b;">encoder</span> = model.encoder
encoder.<span style="color: #8f0075;">eval</span>()

<span style="color: #5317ac;">with</span> torch.no_grad():
    <span style="color: #5317ac;">for</span> batch_features <span style="color: #5317ac;">in</span> <span style="color: #00538b;">test_loader</span>:
        batch_features = batch_features[0].to(device)
        <span style="color: #00538b;">test_examples</span> = batch_features.view(-1, 784)
        <span style="color: #00538b;">hl</span> = encoder(test_examples)
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orgd504263" class="outline-2">
<h2 id="orgd504263">Manifold learners</h2>
<div class="outline-text-2" id="text-orgd504263">
<p>
A number of manifold learner are available trough different
libraries: We'll use
</p>

<ul class="org-ul">
<li>scikit learn for Isomap (global manifold focus)</li>
<li>umap for umap (local manifold focus, good at global manifold)</li>
<li>scikit learn for t-SNE (local manifold focus)</li>
</ul>


<p>
To install umap we can do
</p>
<div class="org-src-container">
<pre class="src src-sh">pip install umap umap-learn
</pre>
</div>
</div>


<div id="outline-container-org6a208a5" class="outline-3">
<h3 id="org6a208a5">UMAP: Uniform Manifold Approximation and Projection</h3>
<div class="outline-text-3" id="text-org6a208a5">
<p>
As already stated this algorithm finds a low dimensional embedding
of the data that approximates an underlying manifold, has a focus
on the local manifold while still providing a good rapresentation
of the global structure.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #5317ac;">import</span> umap

<span style="color: #00538b;">umap_manifold</span> = umap.UMAP(
    random_state=0,
    metric=<span style="color: #2544bb;">'euclidean'</span>,
    n_components=10,
    n_neighbors=20,
    min_dist=0.0)
</pre>
</div>
</div>
</div>

<div id="outline-container-org9a602fb" class="outline-3">
<h3 id="org9a602fb">t-SNE: t-distributed Stochastic Neighbor Embedding</h3>
<div class="outline-text-3" id="text-org9a602fb">
<p>
Is a non-linear method with the objective of optimize for local
distances when creating the embedding. To do so we'll try to use
the Multicore TSNE library as it complies to the sklearn api and is
generally faster, will fall back to sklearn if the installation
results in some problems.
</p>
<div class="org-src-container">
<pre class="src src-sh">pip install git+https://github.com/jorvis/Multicore-TSNE || pip install sklearn
</pre>
</div>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #5317ac;">try</span>:
  <span style="color: #5317ac;">from</span> MulticoreTSNE <span style="color: #5317ac;">import</span> MulticoreTSNE <span style="color: #5317ac;">as</span> TSNE
<span style="color: #5317ac;">except</span>:
  <span style="color: #5317ac;">from</span> sklearn.manifold <span style="color: #5317ac;">import</span> TSNE

tsne_manifold = TSNE(
  n_components=2,
  n_jobs=16,
  random_state=0)

<span style="color: #505050;"># </span><span style="color: #505050;">The fit outputs some warnings as current (9 Jan 2023), but I choose</span>
<span style="color: #505050;"># </span><span style="color: #505050;">to ignore them since they are notices about future changes in the</span>
<span style="color: #505050;"># </span><span style="color: #505050;">API (default parameters)</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-org9772194" class="outline-3">
<h3 id="org9772194">Isomap</h3>
<div class="outline-text-3" id="text-org9772194">
<p>
Isomap is another non-linear method with the objective to optimze
for global manifold rather than local. Is included in the
experiment in order to validate the idea that the embedding of a
local manifold is better suited for this task (set to 10 components
instead than the 2 components we set the other 2, this is because
isomap with fewer components was observed to perform even worse
than shallow clustering alone). At this point we have to install
sklearn if we didn't alredy
</p>
<div class="org-src-container">
<pre class="src src-sh">pip install sklearn
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #5317ac;">from</span> sklearn.manifold <span style="color: #5317ac;">import</span> Isomap

<span style="color: #00538b;">isomap_manifold</span> = Isomap(
    n_components=10,
    n_neighbors=5)
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org4f3d451" class="outline-2">
<h2 id="org4f3d451">Clustering</h2>
<div class="outline-text-2" id="text-org4f3d451">
<p>
Final passage of our process is the clustering of the embedded data,
to do so we'll try different methods: the Gaussian Mixture Models
(GMM). It is a shallow method for clustering, tuned in order to
build 10 clusters (number of digits in the dataset) and set to a
random state of 0 for reproducibility.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #5317ac;">from</span> sklearn.mixture <span style="color: #5317ac;">import</span> GaussianMixture
<span style="color: #5317ac;">from</span> sklearn.cluster <span style="color: #5317ac;">import</span> KMeans

<span style="color: #00538b;">gmm</span> = GaussianMixture(
    covariance_type=<span style="color: #2544bb;">'full'</span>,
    n_components=10,
    random_state=0)

km = KMeans(
    init=<span style="color: #2544bb;">'k-means++'</span>,
    n_clusters=10,
    random_state=0,
    n_init=20)
</pre>
</div>
</div>

<div id="outline-container-orgf559509" class="outline-3">
<h3 id="orgf559509">Metrics</h3>
<div class="outline-text-3" id="text-orgf559509">
<p>
In order to evaluate the resulting clustering we'll rely on the
notion of accuracy, Normalized mutual information (NMI) and
Adjusted Random Index (ARI). While ARI and NMI are easily
calculated trough the metrics module of the scikit library, for the
accuracy we have to define the measure ourselves.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #5317ac;">from</span> scipy.optimize <span style="color: #5317ac;">import</span> linear_sum_assignment

<span style="color: #5317ac;">def</span> <span style="color: #721045;">linear_assignment</span>(cost_matrix):
    <span style="color: #00538b;">x</span>, <span style="color: #00538b;">y</span> = linear_sum_assignment(cost_matrix)
    <span style="color: #5317ac;">return</span> np.array(<span style="color: #8f0075;">list</span>(<span style="color: #8f0075;">zip</span>(x, y)))

<span style="color: #5317ac;">def</span> <span style="color: #721045;">best_cluster_fit</span>(y_true, y_pred):
    <span style="color: #00538b;">y_true</span> = y_true.astype(np.int64)
    <span style="color: #00538b;">D</span> = <span style="color: #8f0075;">max</span>(y_pred.<span style="color: #8f0075;">max</span>(), y_true.<span style="color: #8f0075;">max</span>()) + 1
    <span style="color: #00538b;">w</span> = np.zeros((D, D), dtype=np.int64)
    <span style="color: #5317ac;">for</span> i <span style="color: #5317ac;">in</span> <span style="color: #8f0075;">range</span>(y_pred.size):
        <span style="color: #00538b;">w</span>[<span style="color: #00538b;">y_pred</span>[i], <span style="color: #00538b;">y_true</span>[i]] += 1

    ind = linear_assignment(w.<span style="color: #8f0075;">max</span>() - w)
    best_fit = []
    <span style="color: #5317ac;">for</span> i <span style="color: #5317ac;">in</span> <span style="color: #8f0075;">range</span>(y_pred.size):
        <span style="color: #5317ac;">for</span> j <span style="color: #5317ac;">in</span> <span style="color: #8f0075;">range</span>(<span style="color: #8f0075;">len</span>(ind)):
            <span style="color: #5317ac;">if</span> ind[j][0] == y_pred[i]:
                best_fit.append(ind[j][1])
    <span style="color: #5317ac;">return</span> best_fit, ind, w


<span style="color: #5317ac;">def</span> <span style="color: #721045;">cluster_acc</span>(y_true, y_pred):
    <span style="color: #00538b;">_</span>, <span style="color: #00538b;">ind</span>, <span style="color: #00538b;">w</span> = best_cluster_fit(y_true, y_pred)
    <span style="color: #5317ac;">return</span> <span style="color: #8f0075;">sum</span>([w[i, j] <span style="color: #5317ac;">for</span> i, j <span style="color: #5317ac;">in</span> ind]) * 1.0 / y_pred.size
</pre>
</div>
</div>
</div>

<div id="outline-container-org5f5f4c7" class="outline-3">
<h3 id="org5f5f4c7">Visualization</h3>
<div class="outline-text-3" id="text-org5f5f4c7">
<p>
In order to visualize the underlying manifold and the cluster
information we'll rely on an utility function an we'll plot our
results for each run (shallow clusters and clustering of embedded
manifolds)
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #5317ac;">import</span> seaborn <span style="color: #5317ac;">as</span> sns
<span style="color: #5317ac;">import</span> pandas <span style="color: #5317ac;">as</span> pd

<span style="color: #5317ac;">def</span> <span style="color: #721045;">plot</span>(x, y, plot_id, names=<span style="color: #0000c0;">None</span>):
    viz_df = pd.DataFrame(data=x[:5000])
    viz_df[<span style="color: #2544bb;">'Label'</span>] = y[:5000]
    <span style="color: #5317ac;">if</span> names <span style="color: #5317ac;">is</span> <span style="color: #5317ac;">not</span> <span style="color: #0000c0;">None</span>:
        viz_df[<span style="color: #2544bb;">'Label'</span>] = viz_df[<span style="color: #2544bb;">'Label'</span>].<span style="color: #8f0075;">map</span>(names)

    plt.subplots(figsize=(8, 5))
    plt.title(plot_id)
    sns.scatterplot(x=0, y=1, 
                    hue=<span style="color: #2544bb;">'Label'</span>, legend=<span style="color: #2544bb;">'full'</span>, 
                    hue_order=<span style="color: #8f0075;">sorted</span>(viz_df[<span style="color: #2544bb;">'Label'</span>].unique()),
                    palette=sns.color_palette(<span style="color: #2544bb;">"hls"</span>, n_colors=10),
                    alpha=.5,
                    data=viz_df)

    plt.ylabel(<span style="color: #2544bb;">""</span>)
    plt.xlabel(<span style="color: #2544bb;">""</span>)
    plt.tight_layout()
    plt.show()
    plt.clf()
</pre>
</div>
</div>
</div>

<div id="outline-container-orgd486a1a" class="outline-3">
<h3 id="orgd486a1a">Measures</h3>
<div class="outline-text-3" id="text-orgd486a1a">
<p>
We'll now proceed to mesure the accuracy and nmi of our model and
compare it againsta a basic clustering algorithm. To do so we will:
</p>
<ul class="org-ul">
<li>run two basic clusterings (gmm and kmeans) in high dimensions</li>
<li>run each manifold technique and for every one of them cluster in
high dimensions</li>
<li>visualize the embedded data in high dimension</li>
</ul>

<p>
First let's reshape our test loader to get all the data at once,
and initialize a list to contain our results
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #5317ac;">from</span> sklearn <span style="color: #5317ac;">import</span> metrics

<span style="color: #505050;"># </span><span style="color: #505050;">loading all the tests at once</span>
<span style="color: #00538b;">test_loader</span> = torch.utils.data.DataLoader(
    test_data, 
    batch_size=10000, 
    shuffle=<span style="color: #0000c0;">False</span>
)

res = []
</pre>
</div>

<p>
We can now try shallow clustering, and see how it performs
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #5317ac;">with</span> torch.no_grad():
    <span style="color: #5317ac;">for</span> x, y <span style="color: #5317ac;">in</span> <span style="color: #00538b;">test_loader</span>:
        x = x.view(-1, 784).cpu()
        <span style="color: #00538b;">y</span> = np.asarray(y.numpy())

      gmm.fit(x)
      <span style="color: #00538b;">y_pred_prob</span> = gmm.predict_proba(x)
      <span style="color: #00538b;">y_pred</span> = y_pred_prob.argmax(1)

      <span style="color: #00538b;">y_pred</span> = np.asarray(y_pred)

      <span style="color: #00538b;">acc</span> = np.<span style="color: #8f0075;">round</span>(cluster_acc(y, y_pred), 5)
      <span style="color: #00538b;">nmi</span> = np.<span style="color: #8f0075;">round</span>(metrics.normalized_mutual_info_score(y, y_pred), 5)
      <span style="color: #00538b;">ari</span> = np.<span style="color: #8f0075;">round</span>(metrics.adjusted_rand_score(y, y_pred), 5)

      <span style="color: #00538b;">res</span> += [(<span style="color: #2544bb;">'GMM'</span>, acc, nmi, ari)]

      <span style="color: #00538b;">y_pred</span> = np.asarray(km.fit_predict(x))

      <span style="color: #00538b;">acc</span> = np.<span style="color: #8f0075;">round</span>(cluster_acc(y, y_pred), 5)
      <span style="color: #00538b;">nmi</span> = np.<span style="color: #8f0075;">round</span>(metrics.normalized_mutual_info_score(y, y_pred), 5)
      <span style="color: #00538b;">ari</span> = np.<span style="color: #8f0075;">round</span>(metrics.adjusted_rand_score(y, y_pred), 5)

      <span style="color: #00538b;">res</span> += [(<span style="color: #2544bb;">'Km'</span>, acc, nmi, ari)]
</pre>
</div>

<p>
We'll define an utility function to print a pretty table with our
results
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #5317ac;">def</span> <span style="color: #721045;">print_res</span>(res):
  <span style="color: #8f0075;">format</span> = <span style="color: #2544bb;">"{:&lt;8} {:&lt;15} {:&lt;10} {:&lt;10}"</span>
  <span style="color: #8f0075;">print</span>(<span style="color: #8f0075;">format</span>.<span style="color: #8f0075;">format</span>(<span style="color: #2544bb;">"name"</span>, <span style="color: #2544bb;">"ACC"</span>, <span style="color: #2544bb;">"NMI"</span>, <span style="color: #2544bb;">"ARI"</span>))
  <span style="color: #5317ac;">for</span> name, acc, nmi, ari <span style="color: #5317ac;">in</span> res:
    <span style="color: #8f0075;">print</span>(<span style="color: #8f0075;">format</span>.<span style="color: #8f0075;">format</span>(name, acc, nmi, ari))

print_res(res)
</pre>
</div>
<p>
As we can see the shallow algorithms have a pretty bad result, they're not really able to cluster the images in a significative way.
</p>
</div>
</div>

<div id="outline-container-orgb03efa0" class="outline-3">
<h3 id="orgb03efa0">Manifolds</h3>
<div class="outline-text-3" id="text-orgb03efa0">
<p>
In order to consistently run our mesures we'll first define an
helper function that
</p>
<ul class="org-ul">
<li>takes the test data: x for the images (viewed as a
784-dimensional vector) and y for the label (digit from 0 to 9)</li>
<li>encodes it trough our autoencoder, resulting in a vector of
dimensionality 2000 (passing trough a bottleneck of
dimension 500)</li>
<li>passes the encoding trough a manifold learner (different each
time)</li>
<li>plots the embedded data, coloring it according to the cluster</li>
<li>returns the accuracy and nmi mesures along with the name (this is
done because we might want to run the evaluation of the model on
different batch of data, the returned value is a vector of tuples
rapresenting for each batch the accuracy and nmi)</li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #5317ac;">def</span> <span style="color: #721045;">cluster_embedded</span>(manifold, name):
  <span style="color: #00538b;">res</span> = []
  <span style="color: #5317ac;">with</span> torch.no_grad():
    <span style="color: #5317ac;">for</span> x, y <span style="color: #5317ac;">in</span> <span style="color: #00538b;">test_loader</span>:
        x = x.view(-1, 784).to(device)
        <span style="color: #00538b;">y</span> = np.asarray(y.numpy())

        <span style="color: #00538b;">hl</span> = encoder(x)

        <span style="color: #00538b;">hle</span> = manifold.fit_transform(hl.cpu())
        gmm.fit(hle)

        <span style="color: #00538b;">y_pred_prob</span> = gmm.predict_proba(hle)
        <span style="color: #00538b;">y_pred</span> = y_pred_prob.argmax(1)
        <span style="color: #00538b;">y_pred</span> = np.asarray(y_pred)

        <span style="color: #00538b;">acc</span> = np.<span style="color: #8f0075;">round</span>(cluster_acc(y, y_pred), 5)
        <span style="color: #00538b;">nmi</span> = np.<span style="color: #8f0075;">round</span>(metrics.normalized_mutual_info_score(y, y_pred), 5)
        <span style="color: #00538b;">ari</span> = np.<span style="color: #8f0075;">round</span>(metrics.adjusted_rand_score(y, y_pred), 5)

        plot(hle, y, f<span style="color: #2544bb;">"</span>{name}<span style="color: #2544bb;"> true labels"</span>)
        <span style="color: #00538b;">best_fit</span>, <span style="color: #00538b;">_</span>, <span style="color: #00538b;">_</span> = best_cluster_fit(y, y_pred)
        plot(hle, best_fit, f<span style="color: #2544bb;">"</span>{name}<span style="color: #2544bb;"> predicted labels"</span>)

        <span style="color: #00538b;">res</span> += [(name, acc, nmi, ari)]
  <span style="color: #5317ac;">return</span> res
</pre>
</div>
</div>
<div id="outline-container-orgdd7b1e6" class="outline-4">
<h4 id="orgdd7b1e6">UMAP manifold</h4>
<div class="outline-text-4" id="text-orgdd7b1e6">
<p>
We'll apply <code>umap</code> to our encoded space and then cluster it with
<code>gmm</code>, mesure its accuracy and nmi and plot the data on a 2d map.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #00538b;">res</span> += cluster_embedded(umap_manifold, <span style="color: #2544bb;">'umap'</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-org8161276" class="outline-4">
<h4 id="org8161276">t-SNE manifold</h4>
<div class="outline-text-4" id="text-org8161276">
<p>
We'll apply <code>t-SNE</code> to our encoded space and cluster it with
<code>gmm</code>, mesure its accuracy and nmi and plot the data on a 2d map.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #00538b;">res</span> += cluster_embedded(tsne_manifold, <span style="color: #2544bb;">'t-SNE'</span>)
</pre>
</div>
</div>
</div>

<div id="outline-container-orgc75f380" class="outline-4">
<h4 id="orgc75f380">ISOMAP manifold</h4>
<div class="outline-text-4" id="text-orgc75f380">
<p>
We'll apply <code>isomap</code> to our encoded space and cluster it with
<code>gmm</code>, mesure its accuracy and nmi and plot the data on a 2d map.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #00538b;">res</span> += cluster_embedded(isomap_manifold, <span style="color: #2544bb;">'isomap'</span>)
</pre>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-orgb660f6d" class="outline-2">
<h2 id="orgb660f6d">Results</h2>
<div class="outline-text-2" id="text-orgb660f6d">
<p>
Finally we can print our total results.
</p>

<p>
As we can see our experiment confirmed the results of the original
paper. The model outperforms shallow clustering, as <code>gmm</code> and
<code>kMeans</code> reach a far lower accuracy and normalized mutual
information score than the model, regardless of the manifold
embedding we used. Another observation is that UMAP (focused on
local manifold, but with a better preservation of global manifold)
is the best across the bord, outperforming t-SNE and isomap.
</p>

<div class="org-src-container">
<pre class="src src-python">print_res(res)
</pre>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Luca Zaninotto</p>
<p class="date">Created: 2023-03-16 Thu 15:57</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
